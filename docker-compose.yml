version: "3.8"
services:
  ################################
  # Docker Registry
  ##################################
  registry:
    hostname: registry
    domainname: test
    container_name: registry
    image: registry:2.7.1
    network_mode: "host" # Host here in OsX is the VM, in Linux it is the actual host...
    # networks:
    #   backend:

  ################################
  # DNS
  ################################
  # dns:
  #   hostname: dns
  #   domainname: test
  #   container_name: dns
  #   build:
  #     context: ./services/dns/
  #     dockerfile: Dockerfile
  #   # command: ["-k", "--except-interface=lo", "--bind-interfaces", "--interface=docker0"]
  #   command: ["-k", "--log-queries"]
  #   ports:
  #     - 53:53/tcp
  #     - 53:53/udp
  #   volumes:
  #     - ./config/dns/dnsmasq.conf:/etc/dns/dnsmasq.conf:ro
  #   # network_mode: "host"
  #   networks:
  #     backend:
  #       ipv4_address: 172.18.255.0
  #   cap_add:
  #     - NET_ADMIN

  dns:
    hostname: dns
    domainname: test
    container_name: dns
    build:
      context: ./services/dns/
      dockerfile: DockerfileCoreDns
    # command: ["-k", "--except-interface=lo", "--bind-interfaces", "--interface=docker0"]
    ports:
      - 53:53/tcp
      - 53:53/udp
      - 9153:9153 # Prometheus metrics
    volumes:
      - ./config/dns/Corefile:/etc/coredns/Corefile:ro
    # network_mode: "host"
    dns: [8.8.8.8, 1.1.1.1]
    networks:
      backend:
        ipv4_address: 172.18.255.0
    cap_add:
      - NET_ADMIN

  ################################
  # API Gateway
  # Proxy, Load Balancer
  ################################

  gateway:
    container_name: gateway
    hostname: gateway
    domainname: test
    image: kong:alpine
    environment:
      - KONG_DATABASE=off
      # - KONG_LOG_LEVEL=debug
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml
      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl
    volumes:
      - ./config/kong/kong.yml:/usr/local/kong/declarative/kong.yml:ro
    ports:
      - "8000:8000" # Proxy
      - "8443:8443" # Proxy SSL
      - "8001:8001" # Admin
      - "8444:8444" # Admin SSL
    networks:
      backend:

  # kong-migration:
  #   image: kong:alpine
  #   command: "kong migrations bootstrap"
  #   restart: on-failure
  #   environment:
  #     KONG_PG_HOST: kong_gateway_db
  #   links:
  #     - kong_gateway_db
  #   depends_on:
  #     - kong_gateway_db

  # kong_gateway_db:
  #   container_name: kong_db
  #   hostname: "kong_db"
  #   image: postgres:13-alpine
  #   environment:
  #     - POSTGRES_USER=kong
  #     - POSTGRES_DB=kong
  #   ports:
  #     - "5432:5432"

  #################################
  # Reverse Proxy and Load Balancer
  #################################

  # nginx:
  #   container_name: nginx
  #   hostname: nginx
  #   image : nginx:1.19.2-alpine
  #   ports :
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./config/certs:/etc/nginx/certs:ro
  #     - ./config/nginx.conf:/etc/nginx/conf.d/nginx.conf:ro

  ################################
  # Applications
  ################################

  api:
    container_name: api
    hostname: api
    domainname: test
    build:
      context: ./services/api/
      dockerfile: Dockerfile
    # logging:
    #   driver: "fluentd"
    #   options:
    #     fluentd-address: localhost:24224
    #     tag: api.access
    environment:
      - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
    ports:
      - "3000:3000"
    dns: 172.18.255.0
    networks:
      backend:

  db:
    container_name: database
    hostname: database
    domainname: test
    image: postgres:13-alpine
    environment:
      - POSTGRES_PASSWORD=password
    networks:
      backend:

  # redis-api-cache:
  #   image: redis:alpine
  #   container_name: redis-api-cache
  #   ports:
  #   - 6379:6379

  # ui:
  #   build:
  #       context: ./backend/
  #       dockerfile: Dockerfile
  #   container_name: ui
  #   environment:
  #       - HOST=mongo
  #   ports:
  #       - '8080:8080'
  #   depends_on: [gateway]

  ################################
  # Monitoring/Metrics
  ################################

  # Monitoring Dashboard and UI
  prometheus:
    image: prom/prometheus:latest
    hostname: prometheus
    domainname: test
    container_name: prometheus
    ports:
      - 9090:9090
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - cadvisor
      - gateway
    dns: 172.18.255.0
    networks:
      - backend

  grafana:
    image: grafana/grafana
    hostname: grafana
    domainname: test
    container_name: grafana
    ports:
      - 3001:3000
    dns: 172.18.255.0
    networks:
      - backend

  # Container metrics from docker directly
  cadvisor:
    hostname: cadvisor
    domainname: test
    image: gcr.io/google-containers/cadvisor:latest
    container_name: cadvisor
    ports:
      - 8080:8080
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    depends_on:
      - redis-cadvisor
    dns: 172.18.255.0
    networks:
      backend:

  # Required by cadvisor
  redis-cadvisor:
    hostname: redis-cadvisor
    domainname: test
    image: redis:alpine
    container_name: redis-cadvisor
    ports:
      - 6379:6379
    dns: 172.18.255.0
    networks:
      backend:

  ################################
  # Distributed Tracing
  ################################

  # Jaeger
  jaeger-all-in-one:
    container_name: jaeger
    hostname: jaeger
    domainname: test
    image: jaegertracing/all-in-one:latest
    command: ["--collector.zipkin.http-port=9411"]
    ports:
      - "16686:16686" # Jaeger UI
      - "14268" # Jaeger Collector port
      - "14250" # collector accept model.proto
      - "9411:9411" # Zipkin compatible endpoint
    dns: 172.18.255.0
    networks:
      backend:

  # Zipkin
  # zipkin-all-in-one:
  #   container_name: zipkin
  #   hostname: zipkin
  #   image: openzipkin/zipkin:latest
  #   ports:
  #     - "9411:9411"

  # Open Telemetry Agent
  otel-agent:
    container_name: otel-agent
    domainname: test
    image: otel/opentelemetry-collector
    command: ["--config=/etc/otel-agent-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./config/open-telemetry/otel-agent-config.yaml:/etc/otel-agent-config.yaml:ro
    ports:
      - "1777:1777" # pprof extension
      - "8887:8888" # Prometheus metrics exposed by the agent
      - "14268" # Jaeger receiver
      - "55678" # OpenCensus receiver
      - "55679:55679" # zpages extension
      - "13133" # health_check
    depends_on:
      - otel-collector
    dns: 172.18.255.0
    networks:
      backend:

  # Open Telemetry Collector
  otel-collector:
    domainname: test
    container_name: otel-collector
    image: otel/opentelemetry-collector
    command: ["--config=/etc/otel-collector-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./config/open-telemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "1888:1888" # pprof extension
      - "8888:8888" # Prometheus metrics exposed by the collector
      - "8889:8889" # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "55678" # OpenCensus receiver
      - "55680:55679" # zpages extension
    depends_on:
      - jaeger-all-in-one
      # - zipkin-all-in-one
    dns: 172.18.255.0
    networks:
      backend:

  ################################
  # Mock Data
  ################################

  # Synthetic Load Generators
  # jaeger-emitter:
  #   image: omnition/synthetic-load-generator:1.0.25
  #   environment:
  #     - JAEGER_COLLECTOR_URL=http://otel-agent:14268
  #   depends_on:
  #     - otel-agent

  # zipkin-emitter:
  #   image: omnition/synthetic-load-generator:1.0.25
  #   environment:
  #     - ZIPKINV2_JSON_URL=http://otel-agent:9411/api/v2/spans
  #   depends_on:
  #     - otel-agent

  ################################
  # Logging and Visualization
  ################################

  # Vector -> Elasticsearch -> kibana

  # Vector a tiny Rust based alternative to fluentd/fluentbit
  vector:
    hostname: vector
    domainname: test
    container_name: vector
    image: timberio/vector:latest-alpine
    volumes:
      - ./config/vector/vector.toml:/etc/vector/vector.toml:ro
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "9000:9000"
    dns: 172.18.255.0
    networks:
      backend:

  # Full text search provider
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  elasticsearch:
    container_name: elasticsearch
    hostname: elasticsearch
    domainname: test
    image: elasticsearch:7.9.1
    environment:
      - node.name=elasticsearch
      - cluster.name=es-docker-cluster
      - bootstrap.memory_lock=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.type=single-node"
      # - bootstrap.memory_lock=true
      - xpack.security.enabled=false # Disable security plugins ie. Login
    # volumes:
    # - ./config/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    ports:
      - 9200:9200
      - 9300:9300
    expose:
      - "9200"
    dns: 172.18.255.0
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      backend:

  # UI interface for full text on logs and management of logging/metrics
  kibana:
    container_name: kibana
    hostname: kibana
    domainname: test
    image: kibana:7.9.1
    dns: 172.18.255.0
    ports:
      - 5601:5601
    networks:
      backend:

  ################################
  # Packet Capture
  ################################
  tshark:
    hostname: tshark
    domainname: test
    container_name: tshark
    build:
      context: ./services/tshark/
      dockerfile: Dockerfile
    
    # Use the capture command below if required or run directly in the container using exec
    command: ["--help"]
    # command: ["-i", "any", "-w", "./captures/capture.pcap"]

    # command: ["-i", "any", "-w", "./captures/pipe"] # standard file output
    # output to FIFO then pipe into wireshark
    # command: [sh, -c, "rm -f ./captures/pipe && mkfifo ./captures/pipe && tshark -i any -w ./captures/pipe"]
    volumes:
      - ./services/tshark/captures/:/usr/src/app/captures/
    # https://docs.docker.com/compose/compose-file/#network_mode
    network_mode: "host" # Host here in OsX is the VM, in Linux it is the actual host...

volumes:
  database:

networks:
  backend-no-internet:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: 'false'
      # com.docker.network.bridge.enable_icc: 'false'
    internal: true
  backend:
    driver: "bridge"
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/16
