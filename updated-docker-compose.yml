version: '3.8'
services:

  ################################
  # API Gateway
  # Proxy, Load Balancer
  ################################

  gateway:
    container_name: gateway
    hostname: "gateway"
    image: kong:alpine
    environment:
      - KONG_DATABASE=off
      # - KONG_LOG_LEVEL=debug
      - KONG_PROXY_ACCESS_LOG=/dev/stdout
      - KONG_ADMIN_ACCESS_LOG=/dev/stdout
      - KONG_PROXY_ERROR_LOG=/dev/stderr
      - KONG_ADMIN_ERROR_LOG=/dev/stderr
      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml
      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl
    volumes:
      - ./config/kong/kong.yml:/usr/local/kong/declarative/kong.yml:ro
    ports:
        - '8000:8000' # Proxy
        - '8443:8443' # Proxy SSL
        - '8001:8001' # Admin
        - '8444:8444' # Admin SSL
    # healthcheck:
    #   test: ["CMD", "kong", "health"]
    #   interval: 10s
    #   timeout: 10s
    #   retries: 10
    # links: 
    #     - "kong_gateway_db"

  # kong-migration:
  #   image: kong:alpine
  #   command: "kong migrations bootstrap"
  #   restart: on-failure
  #   environment:
  #     KONG_PG_HOST: kong_gateway_db
  #   links:
  #     - kong_gateway_db
  #   depends_on:
  #     - kong_gateway_db

  # kong_gateway_db:
  #   container_name: kong_db
  #   hostname: "kong_db"
  #   image: postgres:13-alpine
  #   environment:
  #     - POSTGRES_USER=kong
  #     - POSTGRES_DB=kong
  #   ports:
  #     - "5432:5432"

  #################################
  # Reverse Proxy and Load Balancer
  #################################

  # nginx: 
  #   container_name: nginx
  #   hostname: nginx
  #   image : nginx:1.19.2-alpine
  #   ports : 
  #     - "80:80" 
  #     - "443:443"
  #   volumes:
  #     - ./config/certs:/etc/nginx/certs:ro
  #     - ./config/nginx.conf:/etc/nginx/conf.d/nginx.conf:ro

  ################################
  # Applications
  ################################

  api:
      container_name: api
      hostname: api
      build:
          context: ./services/api/
          dockerfile: Dockerfile
      logging:
        driver: "json-file"
        # options:
        #   max-size: 10m
        #   max-file: 3 
      # logging:
      #   driver: "fluentd"
      #   options:
      #     fluentd-address: localhost:24224
      #     tag: api.access
      environment:
        - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
      ports:
          - '3000:3000'
      # networks:
      #   - backend

  db:
    image: postgres:13-alpine
    container_name: database
    environment:
      - POSTGRES_PASSWORD=password

  # redis-api-cache:
  #   image: redis:alpine
  #   container_name: redis-api-cache
  #   ports:
  #   - 6379:6379

  # ui:
  #   build:
  #       context: ./backend/
  #       dockerfile: Dockerfile
  #   container_name: ui
  #   environment:
  #       - HOST=mongo
  #   ports:
  #       - '8080:8080'
  #   depends_on: [gateway]

################################
# Monitoring/Metrics
################################

  # Monitoring Dashboard and UI
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - 9090:9090
    command:
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./services/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - cadvisor

  # Container metrics from docker directly
  cadvisor:
    image: gcr.io/google-containers/cadvisor:latest
    container_name: cadvisor
    ports:
    - 8080:8080
    volumes:
    - /:/rootfs:ro
    - /var/run:/var/run:rw
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
    depends_on:
    - redis-cadvisor

  # Required by cadvisor
  redis-cadvisor:
    image: redis:alpine
    container_name: redis-cadvisor
    ports:
    - 6379:6379

################################
# Distributed Tracing
################################

  # Jaeger
  jaeger-all-in-one:
    container_name: jaeger
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"   # Jaeger UI
      - "14268"         # Jaeger Collector port
      - "14250"         # collector accept model.proto
      - "9411:9411"     # Zipkin compatible endpoint

  # Zipkin
  # zipkin-all-in-one:
  #   container_name: zipkin
  #   hostname: zipkin
  #   image: openzipkin/zipkin:latest
  #   ports:
  #     - "9411:9411"

  # Open Telemetry Agent
  otel-agent:
    container_name: otel-agent
    image: otel/opentelemetry-collector
    command: ["--config=/etc/otel-agent-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./config/open-telemetry/otel-agent-config.yaml:/etc/otel-agent-config.yaml:ro
    ports:
      - "1777:1777"   # pprof extension
      - "8887:8888"   # Prometheus metrics exposed by the agent
      - "14268"       # Jaeger receiver
      - "55678"       # OpenCensus receiver
      - "55679:55679" # zpages extension
      - "13133"       # health_check
    depends_on:
      - otel-collector

  # Open Telemetry Collector
  otel-collector:
    container_name: otel-collector
    image: otel/opentelemetry-collector
    command: ["--config=/etc/otel-collector-config.yaml", "${OTELCOL_ARGS}"]
    volumes:
      - ./config/open-telemetry/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "55678"       # OpenCensus receiver
      - "55680:55679" # zpages extension
    depends_on:
      - jaeger-all-in-one
      # - zipkin-all-in-one

  ################################
  # Mock Data
  ################################

  # Synthetic Load Generators
  # jaeger-emitter:
  #   image: omnition/synthetic-load-generator:1.0.25
  #   environment:
  #     - JAEGER_COLLECTOR_URL=http://otel-agent:14268
  #   depends_on:
  #     - otel-agent

  # zipkin-emitter:
  #   image: omnition/synthetic-load-generator:1.0.25
  #   environment:
  #     - ZIPKINV2_JSON_URL=http://otel-agent:9411/api/v2/spans
  #   depends_on:
  #     - otel-agent

  ################################
  # Logging and Visualization
  ################################

  # unifying layer between different types of log inputs and outputs.

  # https://fluentbit.io/documentation/0.8/about/fluentd_and_fluentbit.html
  # fluentd:
  #   hostname: fluentd
  #   container_name: fluentd
  #   # image: fluent/fluentd:v1.11.2-1.0
  #   build:
  #     context: ./services/fluentd/
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./config/fluentd/fluentd.conf:/fluentd/etc/fluentd.conf
  #   ports:
  #     - "24224:24224"
  #     - "24224:24224/udp"
  #   networks:
  #     - backend

  # fluentbit:
  #   hostname: fluentbit
  #   container_name: fluentbit
  #   image: fluent/fluent-bit
  #   # build:
  #   #   context: ./services/fluentd/
  #   #   dockerfile: Dockerfile
  #   # volumes:
  #   #   - ./config/fluentd/fluentd.conf:/fluentd/etc/fluentd.conf
  #   ports:
  #     - "24224:24224"
  #     - "24224:24224/udp"
  #   networks:
  #     - backend

  # Vector a tiny Rust based alternative to fluentd
  vector:
    hostname: vector
    container_name: vector
    image: timberio/vector:latest-alpine
    volumes:
      - ./config/vector/vector.toml:/etc/vector/vector.toml:ro
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "9000:9000"
    # networks:
    #   - backend

  # Reads from docker API and outputs logs to Logstash
  # filebeat:
  #   container_name: filebeat
  #   hostname: filebeat
  #   # also docker.elastic.co/beats/filebeat:7.9.1
  #   image: store/elastic/filebeat:7.9.1
  #   # ports:
  #   #   - "5044:5044"

  #   # https://stackoverflow.com/questions/55421363/filebeat-read-logs-from-a-running-docker-image-on-mac-os
  #   volumes:
  #   - ./config/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
  #   - /var/run/docker.sock:/var/run/docker.sock
  #   - /var/lib/docker/containers:/var/lib/docker/containers:ro
  #   depends_on:
  #     - logstash
  #   links:
  #     - logstash

  # Collects parses and transforms log data
  # logstash:
  #   container_name: logstash
  #   hostname: logstash
  #   image: logstash:7.9.1
  #   ports:
  #     # - "5045:5045" # beats
  #     - "5044:5044"
  #     - "5000:5000/tcp"
  #     - "5000:5000/udp"
  #     - "9600:9600" # API
  #   environment:
  #     LS_JAVA_OPTS: "-Xmx256m -Xms256m"
  #   volumes:
  #     - type: bind
  #       source: ./config/logstash/pipeline
  #       target: /usr/share/logstash/pipeline
  #       read_only: true
  #     - ./config/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
  #   depends_on:
  #     - elasticsearch
  #   links:
  #     - elasticsearch

  # Log Search
  # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  elasticsearch:
    container_name: elasticsearch
    hostname: elasticsearch
    image: elasticsearch:7.9.1
    environment:
      - node.name=elasticsearch
      - cluster.name=es-docker-cluster
      - bootstrap.memory_lock=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "discovery.type=single-node"
      # - bootstrap.memory_lock=true
      - xpack.security.enabled=false # Disable security plugins ie. Login
    # volumes:
    # - ./config/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    ports:
      - 9200:9200
      - 9300:9300
    expose:
      - "9200"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    # networks:
    #   - backend

  # UI interface for full text on logs and management of logging/metrics
  kibana:
    container_name: kibana
    hostname: kibana
    image: kibana:7.9.1
    ports:
      - 5601:5601
    # networks:
    #   - backend

  loki:
    container_name: loki
    hostname: loki
    image: grafana/loki:1.5.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - backend      

volumes:
  database:

networks:
  backend:
    driver: bridge
  frontend: